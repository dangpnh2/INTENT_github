{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "cs_path = '/home/grad24/dangpnh/dangpnh/HSLN-Joint-Sentence-Classification/data/PubMed_20k_RCT/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_format(file_name):\n",
    "    meta_ = []\n",
    "    sents_ = []\n",
    "    labels_ = []\n",
    "    with open(file_name, errors='ignore') as f:\n",
    "\n",
    "        doc_sent_count = 0\n",
    "        for idx, line in enumerate(f):\n",
    "            # start doc\n",
    "            if \"###\" in line:\n",
    "                if doc_sent_count!=0:\n",
    "                    meta_.append(doc_sent_count)\n",
    "                doc_sent_count = 0\n",
    "                continue\n",
    "            if '\\t' in line:\n",
    "                label, sent = line.split('\\t')\n",
    "                sents_.append(sent)\n",
    "                labels_.append(label)\n",
    "                doc_sent_count+=1\n",
    "    meta_.append(doc_sent_count)\n",
    "    return meta_, sents_, labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load intent label for sentence level\n",
    "train_file = cs_path + \"train_clean.txt\"\n",
    "dev_file = cs_path + \"dev_clean.txt\"\n",
    "test_file = cs_path + \"test_clean.txt\"\n",
    "\n",
    "\n",
    "meta_train, sents_train, labels_train = read_text_format(train_file)\n",
    "meta_dev, sents_dev, labels_dev = read_text_format(dev_file)\n",
    "meta_test, sents_test, labels_test = read_text_format(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_ = meta_train+meta_dev+meta_test\n",
    "sents_ = sents_train+sents_dev+sents_test\n",
    "labels_ = labels_train+labels_dev+labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncs = '!\"#$%&\\'()*+,-/:;<=>@[\\]^_`{|}~'\n",
    "stop_words_analogy = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \n",
    "                      \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \n",
    "                      \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \n",
    "                      \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \n",
    "                      \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \n",
    "                      \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \n",
    "                      \"an\", \"the\", \"and\", \"if\", \"or\", \"as\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \n",
    "                      \"into\", \"through\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\",\n",
    "                      \"under\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \n",
    "                      \"any\", \"both\", \"each\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"own\", \"same\", \n",
    "                      \"so\", \"than\", \"too\", \"very\", \"can\", \"just\", \"don\", \"should\"]\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "\n",
    "    s = s.lower().strip()\n",
    "    for c in list(puncs):\n",
    "        s=s.replace(c, \" \")\n",
    "    s = word_tokenize(s)\n",
    "    s = filter(lambda x:x not in stop_words_analogy, s)\n",
    "    s = ' '.join(e for e in s)\n",
    "    s = re.sub(\"(?<!\\S)\\d+(?!\\S)\", \"\", s) ## REMOVE NUMBERS \n",
    "\n",
    "    return s\n",
    "sents_ = [normalizeString(e) for e in sents_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grad24/dangpnh/miniconda3/envs/py36_latest_torch/lib/python3.6/site-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "doc_list_split = [doc.split() for doc in sents_]\n",
    "dictionary = corpora.Dictionary(doc_list_split)\n",
    "dictionary.filter_extremes(no_below=0, no_above=0.9,keep_n=5000)\n",
    "corpus = np.array([dictionary.doc2bow(line) for line in doc_list_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words = []\n",
    "for k, v in dictionary.token2id.items():\n",
    "    list_words.append(k)\n",
    "list_words_idx = {k: v for v, k in enumerate(list_words)}\n",
    "def keep_w(sent):\n",
    "    new_l = []\n",
    "    words = sent.split()\n",
    "    for w in words:\n",
    "        if w in list_words:\n",
    "            new_l.append(w)\n",
    "    return ' '.join(e for e in new_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_list(l, group_size):\n",
    "    \"\"\"\n",
    "    :param l:           list\n",
    "    :param group_size:  size of each group\n",
    "    :return:            Yields successive group-sized lists from l.\n",
    "    \"\"\"\n",
    "    for i in xrange(0, len(l), group_size):\n",
    "        yield l[i:i+group_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sents_ = [keep_w(e) for e in sents_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_doc_len(doc_len, max_len):\n",
    "    assert doc_len > max_len\n",
    "    last_numb = []\n",
    "    if doc_len%max_len:\n",
    "        last_numb = [doc_len%max_len]\n",
    "    return [max_len]*int(doc_len/max_len)#+last_numb\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(meta_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTS = max(meta_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sents_ = []\n",
    "new_labels_ = []\n",
    "new_meta_ = []\n",
    "\n",
    "curr_sent = 0\n",
    "\n",
    "for doc_len in meta_:\n",
    "    \n",
    "    if doc_len > MAX_SENTS:\n",
    "        splitted_doc_len = split_doc_len(doc_len, MAX_SENTS)\n",
    "    else:\n",
    "        splitted_doc_len = [doc_len]\n",
    "    \n",
    "    for sub_doc_len in splitted_doc_len:\n",
    "        total_sent = 0\n",
    "        for idx in range(curr_sent, curr_sent+sub_doc_len):\n",
    "            sent_words = keep_w(sents_[idx])\n",
    "            if len(sent_words)>3:\n",
    "\n",
    "                new_sents_.append(sent_words)\n",
    "                new_labels_.append(labels_[idx])\n",
    "                total_sent+=1\n",
    "\n",
    "        curr_sent+=sub_doc_len\n",
    "        if total_sent>0:\n",
    "            new_meta_.append(total_sent)\n",
    "    \n",
    "assert sum(new_meta_) == len(new_sents_)\n",
    "\n",
    "labels_ = new_labels_\n",
    "meta_ = new_meta_\n",
    "sents_ = new_sents_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(set(labels_))\n",
    "values = list(range(1,1+len(keys)))\n",
    "dict_ = dict(zip(keys, values))\n",
    "\n",
    "int_labels = []\n",
    "for l in labels_:\n",
    "    int_labels.append(dict_[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OBJECTIVE': 1, 'RESULTS': 2, 'BACKGROUND': 3, 'CONCLUSIONS': 4, 'METHODS': 5}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_map = {v-1: k for k, v in dict_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'OBJECTIVE', 1: 'RESULTS', 2: 'BACKGROUND', 3: 'CONCLUSIONS', 4: 'METHODS'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/home/grad24/dangpnh/dangpnh/analogy_vae/data/pubmed/'\n",
    "label_file = save_path+'pubmed.label'\n",
    "labelstr_file = save_path+'pubmed.strlabel'\n",
    "meta_file = save_path+ 'pubmed.meta'\n",
    "text_file = save_path+'pubmed.text'\n",
    "\n",
    "# write labels in integer\n",
    "with open(label_file, 'w') as f:\n",
    "    for line in int_labels:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "# write labels in str\n",
    "with open(labelstr_file, 'w') as f:\n",
    "    for line in labels_:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "# write meta\n",
    "with open(meta_file, 'w') as f:\n",
    "    for line in meta_:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "# write text\n",
    "with open(text_file, 'w') as f:\n",
    "    for line in sents_:\n",
    "        line=line.strip()\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write vocab, .data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_path = '/home/grad24/dangpnh/dangpnh/analogy_vae/data/pubmed20k/'\n",
    "list_words_dict = {k: v for k,v in enumerate(list_words)}\n",
    "inv_list_words_dict = {v: k for k, v in list_words_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## write vocab; .data; vocab file start from 1; data start from 0\n",
    "with open(save_path+\"pubmed20k.vocab\", \"w\") as f:\n",
    "    for k,v in list_words_dict.items():\n",
    "        f.write(f\"{k+1},{v}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(save_path+\"pubmed20k.text\", \"r\") as f:\n",
    "    original_text = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "new_sents = []\n",
    "for sent in original_text:\n",
    "    words = sent.split()\n",
    "    c_words = dict(Counter(words))\n",
    "    data_sent = ','.join([str(inv_list_words_dict[k])+\":\"+str(v) for k,v in c_words.items() if k in inv_list_words_dict])\n",
    "    new_sents.append(data_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path+\"pubmed20k.data\", \"w\") as f:\n",
    "    for i in new_sents:\n",
    "        f.write(f\"{i}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_latest_torch",
   "language": "python",
   "name": "py36_latest_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
